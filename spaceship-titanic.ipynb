{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c18681e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:36.682017Z",
     "iopub.status.busy": "2022-06-21T12:06:36.681546Z",
     "iopub.status.idle": "2022-06-21T12:06:36.696667Z",
     "shell.execute_reply": "2022-06-21T12:06:36.695605Z"
    },
    "papermill": {
     "duration": 0.029754,
     "end_time": "2022-06-21T12:06:36.699420",
     "exception": false,
     "start_time": "2022-06-21T12:06:36.669666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/spaceship-titanic/sample_submission.csv\n",
      "/kaggle/input/spaceship-titanic/train.csv\n",
      "/kaggle/input/spaceship-titanic/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# # Removing current submission files\n",
    "# os.remove(\"/kaggle/working/submission6.csv\")\n",
    "# os.remove(\"/kaggle/working/submission.csv\")\n",
    "def done():\n",
    "    print(\"DONE\") \n",
    "# To check when processes are done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc89474",
   "metadata": {
    "papermill": {
     "duration": 0.006418,
     "end_time": "2022-06-21T12:06:36.712779",
     "exception": false,
     "start_time": "2022-06-21T12:06:36.706361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360149d0",
   "metadata": {
    "papermill": {
     "duration": 0.006179,
     "end_time": "2022-06-21T12:06:36.725564",
     "exception": false,
     "start_time": "2022-06-21T12:06:36.719385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Firstly, let's start by loading the training and testing data. The test data will only have independent variables(i.e X) but the training data will have both dependent(y) and independent(X) variables. The training data contains a table with different features(independent variables) of previous passengers and whether they were transported or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30323028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:36.740174Z",
     "iopub.status.busy": "2022-06-21T12:06:36.739790Z",
     "iopub.status.idle": "2022-06-21T12:06:38.141542Z",
     "shell.execute_reply": "2022-06-21T12:06:38.140364Z"
    },
    "papermill": {
     "duration": 1.411892,
     "end_time": "2022-06-21T12:06:38.144057",
     "exception": false,
     "start_time": "2022-06-21T12:06:36.732165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Importing train and test data\n",
    "X_train_full = pd.read_csv(\"../input/spaceship-titanic/train.csv\")\n",
    "X_test = pd.read_csv(\"../input/spaceship-titanic/test.csv\")\n",
    "\n",
    "# Drop the Name and PassengerId columns. As well as the other columns to prevent target leakage.\n",
    "X_train_full.drop([\"PassengerId\", \"Cabin\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Name\"], axis=1, inplace=True)\n",
    "X_test.drop([\"Cabin\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Name\"], axis=1, inplace=True)\n",
    "\n",
    "# Removing rows who don't have a transported value from training data.\n",
    "X_train_full.dropna(axis=0, subset=[\"Transported\"], inplace=True)\n",
    "y1 = X_train_full.Transported\n",
    "X = X_train_full.drop([\"Transported\"],axis=1, inplace=False)\n",
    "\n",
    "# Converting y from bool to int\n",
    "y = y1.astype(\"int64\")\n",
    "\n",
    "# Normalizing Age\n",
    "X.Age = (X[\"Age\"]-X[\"Age\"].min())/(X[\"Age\"].max()-X[\"Age\"].min())\n",
    "X_test.Age = (X_test[\"Age\"]-X_test[\"Age\"].min())/(X_test[\"Age\"].max()-X_test[\"Age\"].min())\n",
    "\n",
    "# From training data, get a train_test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Categorical and numerical data lists.\n",
    "categorical = [column for column in X.columns if X[column].dtype == \"object\"]\n",
    "numerical = [column for column in X.columns if X[column].dtype in ['int64', 'float64']]\n",
    "\n",
    "done()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e32f06",
   "metadata": {
    "papermill": {
     "duration": 0.006862,
     "end_time": "2022-06-21T12:06:38.158309",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.151447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After the code above, we have 2 dataframes. y contains the output, that is for each row of data was this passanger tranported or not. x contains the inputs for each row minus the useless PassengerId and Name columns which were removed from both the training and testing data because they add no value to the model. \n",
    "\n",
    "After carefull consideration, I think the columns Cabin, RoomService, FoodCourt, ShoppingMall, Spa, VRDeck will cause target leakage. This is because these are sevices that the passanger will use once on board. Hence at the point of data collection, there is no way to know that they are going to use the spa or the foodcourt, nor how many time they will use it, nor how much they will spend on these luxuries. Unless the passanger pays for the possibility of using these services as part of the voyage price, but it doesn't seem so since the values are not the same.\n",
    "\n",
    "\n",
    "Then a test_train split is done on the model. This is done early to avoid train_test leakage eventually when we impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5034228c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.175733Z",
     "iopub.status.busy": "2022-06-21T12:06:38.175159Z",
     "iopub.status.idle": "2022-06-21T12:06:38.193669Z",
     "shell.execute_reply": "2022-06-21T12:06:38.192829Z"
    },
    "papermill": {
     "duration": 0.0307,
     "end_time": "2022-06-21T12:06:38.196244",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.165544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0013_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>True</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0018_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>0.240506</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0019_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>True</td>\n",
       "      <td>55 Cancri e</td>\n",
       "      <td>0.392405</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0021_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>0.481013</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0023_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Destination       Age    VIP\n",
       "0     0013_01      Earth      True  TRAPPIST-1e  0.341772  False\n",
       "1     0018_01      Earth     False  TRAPPIST-1e  0.240506  False\n",
       "2     0019_01     Europa      True  55 Cancri e  0.392405  False\n",
       "3     0021_01     Europa     False  TRAPPIST-1e  0.481013  False\n",
       "4     0023_01      Earth     False  TRAPPIST-1e  0.253165  False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X.head()\n",
    "X_test.head()\n",
    "#print(y_train)\n",
    "#print(categorical)\n",
    "#print(numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f64ac50",
   "metadata": {
    "papermill": {
     "duration": 0.006505,
     "end_time": "2022-06-21T12:06:38.209913",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.203408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea2001",
   "metadata": {
    "papermill": {
     "duration": 0.006452,
     "end_time": "2022-06-21T12:06:38.223173",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.216721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, checking the cardinality of the categorical columns to know if they are worth encoding. Since the data shrinks to 6 uselful columns, the cardinality threshold can be increased to about 20 without getting too large of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3eed5c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.238767Z",
     "iopub.status.busy": "2022-06-21T12:06:38.237893Z",
     "iopub.status.idle": "2022-06-21T12:06:38.248392Z",
     "shell.execute_reply": "2022-06-21T12:06:38.247276Z"
    },
    "papermill": {
     "duration": 0.021429,
     "end_time": "2022-06-21T12:06:38.251306",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.229877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cardinality of categorical data\n",
    "cardinalities = []\n",
    "for column in categorical:\n",
    "    card = X[column].nunique()\n",
    "    cardinalities.append((column, card))\n",
    "\n",
    "ordinal_cols = [col for (col,card) in cardinalities if card==2]\n",
    "onehot_cols = list(set(categorical) - set(ordinal_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf200b",
   "metadata": {
    "papermill": {
     "duration": 0.00686,
     "end_time": "2022-06-21T12:06:38.265736",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.258876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that the data has been prepared, it is time to preprocess it with a pipeline.\n",
    "This pipeline called preprocess will:\n",
    "* Step1: Take care of missing data using the Simple Imputer\n",
    "* Step2: Take care of categorical data by encoding\n",
    "\n",
    "Since the only numerical column is Age, it makes sense to impute 0 for the missing values. It is possible the person is a baby and the guardians didn't know how to input their age since they are probably months old by that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478fd161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.281900Z",
     "iopub.status.busy": "2022-06-21T12:06:38.281003Z",
     "iopub.status.idle": "2022-06-21T12:06:38.447890Z",
     "shell.execute_reply": "2022-06-21T12:06:38.446883Z"
    },
    "papermill": {
     "duration": 0.177681,
     "end_time": "2022-06-21T12:06:38.450452",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.272771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "              \n",
    "# Preprocessing for numerical and categorical data\n",
    "numerical_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "ordinal_encoder = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "                                 ('ordinal', OrdinalEncoder())])\n",
    "onehot_encoder = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), \n",
    "                                 ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "preprocessor = ColumnTransformer(transformers=[('numerical', numerical_imputer, numerical),\n",
    "                                               ('ordinal', ordinal_encoder, ordinal_cols),\n",
    "                                               ('onehot', onehot_encoder, onehot_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690fec0",
   "metadata": {
    "papermill": {
     "duration": 0.006564,
     "end_time": "2022-06-21T12:06:38.463990",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.457426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling and Trainging Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793ac77",
   "metadata": {
    "papermill": {
     "duration": 0.006745,
     "end_time": "2022-06-21T12:06:38.477909",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.471164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*NOTE: All models used are present in the code, but are all commented out except for the one with the best performance*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca76f0",
   "metadata": {
    "papermill": {
     "duration": 0.00722,
     "end_time": "2022-06-21T12:06:38.492242",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.485022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then the model is defined. Using a Regressor with gradient decent method using XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "312bd635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.507973Z",
     "iopub.status.busy": "2022-06-21T12:06:38.507575Z",
     "iopub.status.idle": "2022-06-21T12:06:38.512431Z",
     "shell.execute_reply": "2022-06-21T12:06:38.511305Z"
    },
    "papermill": {
     "duration": 0.015448,
     "end_time": "2022-06-21T12:06:38.514734",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.499286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Creating XGBoost model \n",
    "# from xgboost import XGBRegressor as model\n",
    "# my_model = model(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "\n",
    "# # Creating final pipeline that uses the preprocessing steps and invokes the model\n",
    "# pipeline = Pipeline(steps=[('processor', preprocessor),\n",
    "#                            ('model', my_model)])\n",
    "\n",
    "# # Using the model with the Pipeline to train data and make predictions\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# pred = pipeline.predict(X_valid)\n",
    "# predictions = [round(pred[i]) for i in range(len(pred))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b8be0",
   "metadata": {
    "papermill": {
     "duration": 0.006728,
     "end_time": "2022-06-21T12:06:38.528788",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.522060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Trying a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cef0626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.544111Z",
     "iopub.status.busy": "2022-06-21T12:06:38.543747Z",
     "iopub.status.idle": "2022-06-21T12:06:38.642059Z",
     "shell.execute_reply": "2022-06-21T12:06:38.640570Z"
    },
    "papermill": {
     "duration": 0.110162,
     "end_time": "2022-06-21T12:06:38.645787",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.535625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating LogisticRegression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Creating final pipeline that uses the preprocessing steps and invokes the model\n",
    "pipeline = Pipeline(steps=[('processor', preprocessor),\n",
    "                           ('model', logreg)])\n",
    "\n",
    "# Using the model with the Pipeline to train data and make predictions\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_valid)\n",
    "predictions = [round(pred[i]) for i in range(len(pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c798e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.675422Z",
     "iopub.status.busy": "2022-06-21T12:06:38.673867Z",
     "iopub.status.idle": "2022-06-21T12:06:38.679882Z",
     "shell.execute_reply": "2022-06-21T12:06:38.678876Z"
    },
    "papermill": {
     "duration": 0.026342,
     "end_time": "2022-06-21T12:06:38.685372",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.659030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# model = LogisticRegression(solver='lbfgs')\n",
    "# pipeline = Pipeline(steps=[('processor', preprocessor),\n",
    "#                            ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c86840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.713382Z",
     "iopub.status.busy": "2022-06-21T12:06:38.712588Z",
     "iopub.status.idle": "2022-06-21T12:06:38.718358Z",
     "shell.execute_reply": "2022-06-21T12:06:38.717307Z"
    },
    "papermill": {
     "duration": 0.025585,
     "end_time": "2022-06-21T12:06:38.724275",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.698690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "\n",
    "# model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n",
    "# pipeline = Pipeline(steps=[('processor', preprocessor),\n",
    "#                            ('model', model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba3a1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.752279Z",
     "iopub.status.busy": "2022-06-21T12:06:38.751488Z",
     "iopub.status.idle": "2022-06-21T12:06:38.758356Z",
     "shell.execute_reply": "2022-06-21T12:06:38.757348Z"
    },
    "papermill": {
     "duration": 0.023445,
     "end_time": "2022-06-21T12:06:38.760452",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.737007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "# from xgboost import XGBRegressor, DMatrix\n",
    "\n",
    "# pipeline = Pipeline(steps=[('processor', preprocessor)])\n",
    "# pipeline.fit(X_train,y_train)\n",
    "# pipeline.fit(X_valid,y_valid)\n",
    "# model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "# DMatrix(data=X, enable_categorical=True)\n",
    "# model.fit(X_train, y_train, \n",
    "#              early_stopping_rounds=5, \n",
    "#              eval_set=[(label_X_valid, y_valid)],\n",
    "#              verbose=True)\n",
    "# pred = my_model.predict(X_valid)\n",
    "# predictions = [round(pred[i]) for i in range(len(pred))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9c9f4",
   "metadata": {
    "papermill": {
     "duration": 0.006983,
     "end_time": "2022-06-21T12:06:38.774151",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.767168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53e5de45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.790452Z",
     "iopub.status.busy": "2022-06-21T12:06:38.789839Z",
     "iopub.status.idle": "2022-06-21T12:06:38.797458Z",
     "shell.execute_reply": "2022-06-21T12:06:38.796242Z"
    },
    "papermill": {
     "duration": 0.018008,
     "end_time": "2022-06-21T12:06:38.799546",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.781538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29557216791259344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "score = mae(predictions, y_valid)\n",
    "print(score)\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from statistics import mean\n",
    "# scores = -1 * cross_val_score(pipeline, X, y,\n",
    "#                               cv=5,\n",
    "#                               scoring='neg_mean_absolute_error')\n",
    "# print(scores)\n",
    "# print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45401d3a",
   "metadata": {
    "papermill": {
     "duration": 0.006843,
     "end_time": "2022-06-21T12:06:38.813480",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.806637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generating Output Results for Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65b6ae",
   "metadata": {
    "papermill": {
     "duration": 0.006638,
     "end_time": "2022-06-21T12:06:38.827274",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.820636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Training on Full data, then generating output file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2e8e65e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-21T12:06:38.844241Z",
     "iopub.status.busy": "2022-06-21T12:06:38.843525Z",
     "iopub.status.idle": "2022-06-21T12:06:38.971824Z",
     "shell.execute_reply": "2022-06-21T12:06:38.970628Z"
    },
    "papermill": {
     "duration": 0.141046,
     "end_time": "2022-06-21T12:06:38.975704",
     "exception": false,
     "start_time": "2022-06-21T12:06:38.834658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating output for competition\n",
    "pipeline.fit(X, y)\n",
    "outputs = pipeline.predict(X_test).astype(\"int64\").astype(\"bool\")\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': X_test.PassengerId,\n",
    "                       'Transported': outputs})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.209168,
   "end_time": "2022-06-21T12:06:39.717611",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-21T12:06:25.508443",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
